\documentclass[12pt, a4paper, oneside]{article} % Paper size, default font size and one-sided paper
%\graphicspath{{./Figures/}} % Specifies the directory where pictures are stored
%\usepackage[dcucite]{harvard}
\usepackage{rotating}
\usepackage{amsmath}
\usepackage{setspace}
\usepackage{pdflscape}
\usepackage[flushleft]{threeparttable}
\usepackage{multirow}
\usepackage[comma, sort&compress]{natbib}% Use the natbib reference package - read up on this to edit the reference style; if you want text (e.g. Smith et al., 2012) for the in-text references (instead of numbers), remove 'numbers' 
\usepackage{graphicx}
%\bibliographystyle{plainnat}
\bibliographystyle{agsm}
\usepackage[colorlinks = true, citecolor = blue, linkcolor = blue]{hyperref}
%\hypersetup{urlcolor=blue, colorlinks=true} % Colors hyperlinks in blue - change to black if annoying
%\renewcommand[\harvardurl]{URL: \url}
\begin{document}
\title{MCMC}
%\author{Rob Hayward\footnote{University of Brighton Business School, Lewes Road, Brighton, BN2 4AT; Telephone 01273 642586.  rh49@brighton.ac.uk}}
\date{\today}
\maketitle
\section*{Introduction acknowledgement and thanks}
This is a work that is developed from Dave Giles excellent and very helpful overview of Markov Chain Monte Carlo (MCMC) methods. The first of four fantastic posts begins \href{http://davegiles.blogspot.co.uk/2014/03/mcmc-for-econometrics-students-i.html}{here}. 

\section{Bayesian Methods}
There is a blog post from Dave Giles that runs through the Bayesian method.  The example looks at a consumption function (data as consump.dat.txt in the Data folder)
\section{Markov Chain and Gibbs sampler}. Dave Giles code is in the R folder and is called Consumption.R.  \href{http://davegiles.blogspot.ca/2012/04/bayesian-consumption-function.html}{Dave Giles blog post on Bayesian method}. 

\section{Markov Chain}
A Markov chain is a stochasic process where the current value depends only on the immeditely preceeding case. It does not depend on anything before that. The Gibbs sampler.  With two parameters $\Theta_1$ and $\Theta_2$, $p(\Theta_1, \Theta_2)$ is the prior pdf and $L(\Theta_1, \Theta_2 | y) = p(y | \Theta_1, \Theta_2)$ is the likelihood function. By Bayes Theory, the posterior pdf is 
\begin{equation}
p(\Theta_1, \Theta_2 | y) \propto p(\Theta_1, \Theta_2)L(\Theta_1, \Theta_2 |y)
\end{equation}


\end{document}
